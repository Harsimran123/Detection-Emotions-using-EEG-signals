{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_l5tQtEE0PJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2787131b-d4c0-4fdf-809a-681219c016eb"
      },
      "source": [
        "pip install git+https://github.com/forrestbao/pyeeg.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/forrestbao/pyeeg.git\n",
            "  Cloning https://github.com/forrestbao/pyeeg.git to /tmp/pip-req-build-5t02pbaw\n",
            "  Running command git clone -q https://github.com/forrestbao/pyeeg.git /tmp/pip-req-build-5t02pbaw\n",
            "Requirement already satisfied (use --upgrade to upgrade): pyeeg==0.4.4 from git+https://github.com/forrestbao/pyeeg.git in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.6/dist-packages (from pyeeg==0.4.4) (1.18.5)\n",
            "Building wheels for collected packages: pyeeg\n",
            "  Building wheel for pyeeg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyeeg: filename=pyeeg-0.4.4-py2.py3-none-any.whl size=28122 sha256=a2021b7e92f82c7d5cff9b36ec4808d7b1398a7b66ba0dcff6fe3bc6543625af\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-5gjtakxf/wheels/2d/3f/ad/106d4fc80b61d1ea1fc18e76e7439fd98aa043d83d58eae741\n",
            "Successfully built pyeeg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWR6L_FJFOTL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf5642ea-330e-456a-e547-9713889394b1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDxDdMI5E0PO"
      },
      "source": [
        "import numpy as np\n",
        "import pyeeg as pe\n",
        "import pickle as pickle\n",
        "import pandas as pd\n",
        "import math\n",
        "\n",
        "from sklearn import svm\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "import os\n",
        "#import tensorflow as tf\n",
        "import time\n",
        "import random\n",
        "random.seed(72)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0SWI7-YE0PR"
      },
      "source": [
        "channel = [1,2,3,4,6,11,13,17,19,20,21,25,29,31] #14 Channels chosen to fit Emotiv Epoch+\n",
        "band = [4,8,12,16,25,45] #5 bands\n",
        "window_size = 256 #Averaging band power of 2 sec\n",
        "step_size = 16 #Each 0.125 sec update once\n",
        "sample_rate = 128 #Sampling rate of 128 Hz\n",
        "subjectList = ['01','02','03']\n",
        "#List of subjects"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8tM4hyKE0PV"
      },
      "source": [
        "def FFT_Processing (sub, channel, band, window_size, step_size, sample_rate):\n",
        "    '''\n",
        "    arguments:  string subject\n",
        "                list channel indice\n",
        "                list band\n",
        "                int window size for FFT\n",
        "                int step size for FFT\n",
        "                int sample rate for FFT\n",
        "    return:     void\n",
        "    '''\n",
        "    meta = []\n",
        "    with open('/content/drive/My Drive/data_preprocessed_python/s' + sub + '.dat', 'rb') as file:\n",
        "\n",
        "        subject = pickle.load(file, encoding='latin1') #resolve the python 2 data problem by encoding : latin1\n",
        "\n",
        "        for i in range (0,40):\n",
        "            # loop over 0-39 trails\n",
        "            data = subject[\"data\"][i]\n",
        "            labels = subject[\"labels\"][i]\n",
        "            start = 0;\n",
        "\n",
        "            while start + window_size < data.shape[1]:\n",
        "                meta_array = []\n",
        "                meta_data = [] #meta vector for analysis\n",
        "                for j in channel:\n",
        "                    X = data[j][start : start + window_size] #Slice raw data over 2 sec, at interval of 0.125 sec\n",
        "                    Y = pe.bin_power(X, band, sample_rate) #FFT over 2 sec of channel j, in seq of theta, alpha, low beta, high beta, gamma\n",
        "                    meta_data = meta_data + list(Y[0])\n",
        "\n",
        "                meta_array.append(np.array(meta_data))\n",
        "                meta_array.append(labels)\n",
        "\n",
        "                meta.append(np.array(meta_array))    \n",
        "                start = start + step_size\n",
        "                \n",
        "        meta = np.array(meta)\n",
        "        np.save('/content/drive/My Drive/data_preprocessed_python/s' + sub, meta, allow_pickle=True, fix_imports=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raQxhU3uE0PZ"
      },
      "source": [
        "for subjects in subjectList:\n",
        "    FFT_Processing (subjects, channel, band, window_size, step_size, sample_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPMI-0VoE0Pc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2f1b444-0868-43fa-8fae-2b954cb033fe"
      },
      "source": [
        "data_training = []\n",
        "label_training = []\n",
        "data_testing = []\n",
        "label_testing = []\n",
        "\n",
        "for subjects in subjectList:\n",
        "\n",
        "    with open('/content/drive/My Drive/data_preprocessed_python/s' + subjects + '.npy', 'rb') as file:\n",
        "        sub = np.load(file,allow_pickle=True)\n",
        "        for i in range (0,sub.shape[0]):\n",
        "            if i % 8 == 0:\n",
        "                data_testing.append(sub[i][0])\n",
        "                label_testing.append(sub[i][1])\n",
        "            else:\n",
        "                data_training.append(sub[i][0])\n",
        "                label_training.append(sub[i][1])\n",
        "           \n",
        "np.save('/content/drive/My Drive/data_preprocessed_python/data_training', np.array(data_training), allow_pickle=True, fix_imports=True)\n",
        "np.save('/content/drive/My Drive/data_preprocessed_python/label_training', np.array(label_training), allow_pickle=True, fix_imports=True)\n",
        "print(\"training dataset:\", np.array(data_training).shape, np.array(label_training).shape)\n",
        "\n",
        "np.save('/content/drive/My Drive/data_preprocessed_python/data_testing', np.array(data_testing), allow_pickle=True, fix_imports=True)\n",
        "np.save('/content/drive/My Drive/data_preprocessed_python/label_testing', np.array(label_testing), allow_pickle=True, fix_imports=True)\n",
        "print(\"testing dataset:\", np.array(data_testing).shape, np.array(label_testing).shape)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training dataset: (51240, 70) (51240, 4)\n",
            "testing dataset: (7320, 70) (7320, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAq1bPtvE0Pg"
      },
      "source": [
        "with open('/content/drive/My Drive/data_preprocessed_python/data_training.npy', 'rb') as fileTrain:\n",
        "    X  = np.load(fileTrain)\n",
        "    \n",
        "with open('/content/drive/My Drive/data_preprocessed_python/label_training.npy', 'rb') as fileTrainL:\n",
        "    Y  = np.load(fileTrainL)\n",
        "    \n",
        "X = normalize(X)\n",
        "Z = np.ravel(Y[:, [1]])\n",
        "\n",
        "Arousal_Train = np.ravel(Y[:, [0]])\n",
        "Valence_Train = np.ravel(Y[:, [1]])\n",
        "Domain_Train = np.ravel(Y[:, [2]])\n",
        "Like_Train = np.ravel(Y[:, [3]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmVsx2naE0Pl"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEybPYtzE0Pp"
      },
      "source": [
        "import pandas as pd\n",
        "import keras.backend as K\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.models import Sequential\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.utils import to_categorical \n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "import timeit\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Flatten, Dense, Dropout\n",
        "from keras.layers.convolutional import Convolution1D, MaxPooling1D, ZeroPadding1D\n",
        "from keras.optimizers import SGD\n",
        "#import cv2, numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmITO93tE0Pr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b7e8b66-e9eb-4b8c-d318-70bd79be9887"
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "y_train = to_categorical(Z)\n",
        "y_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 1., 0., 0.],\n",
              "       [0., 0., 0., ..., 1., 0., 0.],\n",
              "       [0., 0., 0., ..., 1., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7fRA4YfE0Pu"
      },
      "source": [
        "x_train = np.array(X[:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DrtZM04E0Pw"
      },
      "source": [
        "with open('/content/drive/My Drive/data_preprocessed_python/data_testing.npy', 'rb') as fileTrain:\n",
        "    M  = np.load(fileTrain)\n",
        "    \n",
        "with open('/content/drive/My Drive/data_preprocessed_python/label_testing.npy', 'rb') as fileTrainL:\n",
        "    N  = np.load(fileTrainL)\n",
        "\n",
        "M = normalize(M)\n",
        "L = np.ravel(N[:, [1]])\n",
        "\n",
        "Arousal_Test = np.ravel(N[:, [0]])\n",
        "Valence_Test = np.ravel(N[:, [1]])\n",
        "Domain_Test = np.ravel(N[:, [2]])\n",
        "Like_Test = np.ravel(N[:, [3]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcBgwfcSE0Pz"
      },
      "source": [
        "x_test = np.array(M[:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKicnNEYE0P3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51aac0d8-4ce6-439a-cf4b-635a96c50d78"
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "y_test = to_categorical(L)\n",
        "y_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 1., 0., 0.],\n",
              "       [0., 0., 0., ..., 1., 0., 0.],\n",
              "       [0., 0., 0., ..., 1., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkr2bO__E0P6"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_test = scaler.fit_transform(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeuw33ZHE0P9"
      },
      "source": [
        "x_train = x_train.reshape(x_train.shape[0],x_train.shape[1], 1)\n",
        "x_test = x_test.reshape(x_test.shape[0],x_test.shape[1], 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L93vdGQAE0P_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0dcde68-ec4e-438a-82ef-180bed947461"
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(51240, 70, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTZlT_lbE0QB"
      },
      "source": [
        "batch_size = 256\n",
        "num_classes = 10\n",
        "epochs = 200\n",
        "input_shape=(x_train.shape[1], 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhNHFqZKE0QE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ffe9212-77a0-47c3-db31-0f1eaa3077ed"
      },
      "source": [
        "print(input_shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(70, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgvDa3-cE0QH"
      },
      "source": [
        "from keras.layers import Convolution1D, ZeroPadding1D, MaxPooling1D, BatchNormalization, Activation, Dropout, Flatten, Dense\n",
        "from keras.regularizers import l2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ImWTBrUE0QJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c018f55c-22c5-41a5-df1b-eb673d0dccff"
      },
      "source": [
        "model = Sequential()\n",
        "intput_shape=(x_train.shape[1], 1)\n",
        "model.add(Conv1D(128, kernel_size=3,padding = 'same',activation='relu', input_shape=input_shape))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling1D(pool_size=(2)))\n",
        "model.add(Conv1D(128,kernel_size=3,padding = 'same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling1D(pool_size=(2)))\n",
        "#model.add(Conv1D(64,kernel_size=3,padding = 'same', activation='relu'))\n",
        "#model.add(MaxPooling1D(pool_size=(2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='tanh'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(32, activation='tanh'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d (Conv1D)              (None, 70, 128)           512       \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 70, 128)           512       \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, 35, 128)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 35, 128)           49280     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 35, 128)           512       \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 17, 128)           0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 2176)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                139328    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                170       \n",
            "=================================================================\n",
            "Total params: 192,922\n",
            "Trainable params: 192,410\n",
            "Non-trainable params: 512\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h59BbHePE0QM"
      },
      "source": [
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccfj4mzTE0QP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9370abf8-2469-469a-8cf5-c8a88b1f89ab"
      },
      "source": [
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 1.9766 - accuracy: 0.2674\n",
            "Epoch 2/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 1.7463 - accuracy: 0.3364\n",
            "Epoch 3/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 1.6429 - accuracy: 0.3789\n",
            "Epoch 4/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 1.5665 - accuracy: 0.4098\n",
            "Epoch 5/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 1.4994 - accuracy: 0.4395\n",
            "Epoch 6/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 1.4556 - accuracy: 0.4584\n",
            "Epoch 7/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 1.4014 - accuracy: 0.4806\n",
            "Epoch 8/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 1.3654 - accuracy: 0.4983\n",
            "Epoch 9/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 1.3245 - accuracy: 0.5141\n",
            "Epoch 10/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 1.2897 - accuracy: 0.5288\n",
            "Epoch 11/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 1.2625 - accuracy: 0.5413\n",
            "Epoch 12/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 1.2291 - accuracy: 0.5500\n",
            "Epoch 13/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 1.2026 - accuracy: 0.5634\n",
            "Epoch 14/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 1.1877 - accuracy: 0.5686\n",
            "Epoch 15/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 1.1586 - accuracy: 0.5784\n",
            "Epoch 16/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 1.1388 - accuracy: 0.5871\n",
            "Epoch 17/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 1.1160 - accuracy: 0.5972\n",
            "Epoch 18/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 1.1035 - accuracy: 0.5999\n",
            "Epoch 19/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 1.0828 - accuracy: 0.6096\n",
            "Epoch 20/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 1.0687 - accuracy: 0.6133\n",
            "Epoch 21/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 1.0568 - accuracy: 0.6182\n",
            "Epoch 22/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 1.0393 - accuracy: 0.6258\n",
            "Epoch 23/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 1.0317 - accuracy: 0.6298\n",
            "Epoch 24/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 1.0202 - accuracy: 0.6338\n",
            "Epoch 25/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 1.0028 - accuracy: 0.6393\n",
            "Epoch 26/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 1.0021 - accuracy: 0.6395\n",
            "Epoch 27/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.9822 - accuracy: 0.6452\n",
            "Epoch 28/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.9665 - accuracy: 0.6517\n",
            "Epoch 29/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.9609 - accuracy: 0.6560\n",
            "Epoch 30/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.9565 - accuracy: 0.6560\n",
            "Epoch 31/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.9463 - accuracy: 0.6601\n",
            "Epoch 32/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.9406 - accuracy: 0.6616\n",
            "Epoch 33/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.9336 - accuracy: 0.6640\n",
            "Epoch 34/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.9256 - accuracy: 0.6675\n",
            "Epoch 35/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.9149 - accuracy: 0.6694\n",
            "Epoch 36/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.9066 - accuracy: 0.6734\n",
            "Epoch 37/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.8973 - accuracy: 0.6792\n",
            "Epoch 38/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.8937 - accuracy: 0.6780\n",
            "Epoch 39/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.8988 - accuracy: 0.6804\n",
            "Epoch 40/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.8867 - accuracy: 0.6833\n",
            "Epoch 41/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.8807 - accuracy: 0.6849\n",
            "Epoch 42/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.8730 - accuracy: 0.6874\n",
            "Epoch 43/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.8746 - accuracy: 0.6871\n",
            "Epoch 44/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.8597 - accuracy: 0.6914\n",
            "Epoch 45/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.8520 - accuracy: 0.6939\n",
            "Epoch 46/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.8634 - accuracy: 0.6925\n",
            "Epoch 47/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.8445 - accuracy: 0.6972\n",
            "Epoch 48/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.8331 - accuracy: 0.7004\n",
            "Epoch 49/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.8410 - accuracy: 0.6981\n",
            "Epoch 50/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.8310 - accuracy: 0.7024\n",
            "Epoch 51/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.8332 - accuracy: 0.7017\n",
            "Epoch 52/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.8383 - accuracy: 0.6995\n",
            "Epoch 53/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.8196 - accuracy: 0.7067\n",
            "Epoch 54/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.8216 - accuracy: 0.7076\n",
            "Epoch 55/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.8102 - accuracy: 0.7109\n",
            "Epoch 56/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.8102 - accuracy: 0.7098\n",
            "Epoch 57/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.8043 - accuracy: 0.7104\n",
            "Epoch 58/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.8025 - accuracy: 0.7133\n",
            "Epoch 59/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.7960 - accuracy: 0.7172\n",
            "Epoch 60/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.7937 - accuracy: 0.7154\n",
            "Epoch 61/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.7890 - accuracy: 0.7163\n",
            "Epoch 62/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.7760 - accuracy: 0.7217\n",
            "Epoch 63/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.7745 - accuracy: 0.7213\n",
            "Epoch 64/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.7837 - accuracy: 0.7197\n",
            "Epoch 65/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.7783 - accuracy: 0.7220\n",
            "Epoch 66/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.7717 - accuracy: 0.7232\n",
            "Epoch 67/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.7674 - accuracy: 0.7250\n",
            "Epoch 68/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.7567 - accuracy: 0.7285\n",
            "Epoch 69/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.7619 - accuracy: 0.7273\n",
            "Epoch 70/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.7493 - accuracy: 0.7308\n",
            "Epoch 71/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.7582 - accuracy: 0.7276\n",
            "Epoch 72/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.7516 - accuracy: 0.7317\n",
            "Epoch 73/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.7437 - accuracy: 0.7349\n",
            "Epoch 74/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.7403 - accuracy: 0.7347\n",
            "Epoch 75/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.7441 - accuracy: 0.7332\n",
            "Epoch 76/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.7334 - accuracy: 0.7373\n",
            "Epoch 77/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.7219 - accuracy: 0.7413\n",
            "Epoch 78/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.7316 - accuracy: 0.7378\n",
            "Epoch 79/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.7215 - accuracy: 0.7407\n",
            "Epoch 80/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.7258 - accuracy: 0.7401\n",
            "Epoch 81/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.7251 - accuracy: 0.7409\n",
            "Epoch 82/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.7230 - accuracy: 0.7407\n",
            "Epoch 83/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.7234 - accuracy: 0.7423\n",
            "Epoch 84/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.7082 - accuracy: 0.7469\n",
            "Epoch 85/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.7049 - accuracy: 0.7470\n",
            "Epoch 86/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.7059 - accuracy: 0.7470\n",
            "Epoch 87/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.7038 - accuracy: 0.7486\n",
            "Epoch 88/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.7036 - accuracy: 0.7491\n",
            "Epoch 89/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6969 - accuracy: 0.7519\n",
            "Epoch 90/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6974 - accuracy: 0.7516\n",
            "Epoch 91/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6915 - accuracy: 0.7514\n",
            "Epoch 92/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.7016 - accuracy: 0.7488\n",
            "Epoch 93/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6850 - accuracy: 0.7560\n",
            "Epoch 94/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6902 - accuracy: 0.7523\n",
            "Epoch 95/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6873 - accuracy: 0.7548\n",
            "Epoch 96/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6866 - accuracy: 0.7536\n",
            "Epoch 97/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6701 - accuracy: 0.7593\n",
            "Epoch 98/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6737 - accuracy: 0.7592\n",
            "Epoch 99/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6719 - accuracy: 0.7595\n",
            "Epoch 100/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6706 - accuracy: 0.7608\n",
            "Epoch 101/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6674 - accuracy: 0.7623\n",
            "Epoch 102/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6767 - accuracy: 0.7577\n",
            "Epoch 103/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6580 - accuracy: 0.7642\n",
            "Epoch 104/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6599 - accuracy: 0.7636\n",
            "Epoch 105/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6529 - accuracy: 0.7668\n",
            "Epoch 106/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6594 - accuracy: 0.7634\n",
            "Epoch 107/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6566 - accuracy: 0.7653\n",
            "Epoch 108/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6503 - accuracy: 0.7673\n",
            "Epoch 109/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6527 - accuracy: 0.7673\n",
            "Epoch 110/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6557 - accuracy: 0.7672\n",
            "Epoch 111/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6464 - accuracy: 0.7695\n",
            "Epoch 112/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6475 - accuracy: 0.7673\n",
            "Epoch 113/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6456 - accuracy: 0.7694\n",
            "Epoch 114/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6385 - accuracy: 0.7726\n",
            "Epoch 115/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6387 - accuracy: 0.7716\n",
            "Epoch 116/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6389 - accuracy: 0.7732\n",
            "Epoch 117/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6364 - accuracy: 0.7716\n",
            "Epoch 118/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6277 - accuracy: 0.7758\n",
            "Epoch 119/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6233 - accuracy: 0.7779\n",
            "Epoch 120/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6397 - accuracy: 0.7709\n",
            "Epoch 121/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6309 - accuracy: 0.7747\n",
            "Epoch 122/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6214 - accuracy: 0.7776\n",
            "Epoch 123/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6155 - accuracy: 0.7797\n",
            "Epoch 124/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6256 - accuracy: 0.7776\n",
            "Epoch 125/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6217 - accuracy: 0.7780\n",
            "Epoch 126/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6084 - accuracy: 0.7835\n",
            "Epoch 127/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6260 - accuracy: 0.7778\n",
            "Epoch 128/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6156 - accuracy: 0.7801\n",
            "Epoch 129/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6119 - accuracy: 0.7824\n",
            "Epoch 130/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6107 - accuracy: 0.7825\n",
            "Epoch 131/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6170 - accuracy: 0.7796\n",
            "Epoch 132/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6033 - accuracy: 0.7843\n",
            "Epoch 133/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6060 - accuracy: 0.7829\n",
            "Epoch 134/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6039 - accuracy: 0.7853\n",
            "Epoch 135/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5997 - accuracy: 0.7880\n",
            "Epoch 136/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6031 - accuracy: 0.7856\n",
            "Epoch 137/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6049 - accuracy: 0.7883\n",
            "Epoch 138/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5948 - accuracy: 0.7869\n",
            "Epoch 139/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5983 - accuracy: 0.7887\n",
            "Epoch 140/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5953 - accuracy: 0.7879\n",
            "Epoch 141/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5867 - accuracy: 0.7898\n",
            "Epoch 142/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5936 - accuracy: 0.7885\n",
            "Epoch 143/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5899 - accuracy: 0.7929\n",
            "Epoch 144/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5937 - accuracy: 0.7899\n",
            "Epoch 145/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5887 - accuracy: 0.7913\n",
            "Epoch 146/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5801 - accuracy: 0.7927\n",
            "Epoch 147/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5859 - accuracy: 0.7924\n",
            "Epoch 148/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5869 - accuracy: 0.7908\n",
            "Epoch 149/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5729 - accuracy: 0.7962\n",
            "Epoch 150/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5813 - accuracy: 0.7952\n",
            "Epoch 151/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5792 - accuracy: 0.7963\n",
            "Epoch 152/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5791 - accuracy: 0.7950\n",
            "Epoch 153/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5735 - accuracy: 0.7951\n",
            "Epoch 154/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5713 - accuracy: 0.7980\n",
            "Epoch 155/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5803 - accuracy: 0.7941\n",
            "Epoch 156/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5700 - accuracy: 0.7985\n",
            "Epoch 157/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5731 - accuracy: 0.7972\n",
            "Epoch 158/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5630 - accuracy: 0.7995\n",
            "Epoch 159/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5625 - accuracy: 0.7978\n",
            "Epoch 160/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5613 - accuracy: 0.8000\n",
            "Epoch 161/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5650 - accuracy: 0.8009\n",
            "Epoch 162/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5511 - accuracy: 0.8037\n",
            "Epoch 163/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5593 - accuracy: 0.8010\n",
            "Epoch 164/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5593 - accuracy: 0.8004\n",
            "Epoch 165/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5592 - accuracy: 0.8019\n",
            "Epoch 166/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5672 - accuracy: 0.7986\n",
            "Epoch 167/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5524 - accuracy: 0.8041\n",
            "Epoch 168/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5534 - accuracy: 0.8059\n",
            "Epoch 169/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5526 - accuracy: 0.8037\n",
            "Epoch 170/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5513 - accuracy: 0.8037\n",
            "Epoch 171/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5415 - accuracy: 0.8082\n",
            "Epoch 172/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5464 - accuracy: 0.8053\n",
            "Epoch 173/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5436 - accuracy: 0.8065\n",
            "Epoch 174/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5461 - accuracy: 0.8074\n",
            "Epoch 175/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5456 - accuracy: 0.8063\n",
            "Epoch 176/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5407 - accuracy: 0.8077\n",
            "Epoch 177/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5586 - accuracy: 0.8032\n",
            "Epoch 178/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5464 - accuracy: 0.8057\n",
            "Epoch 179/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5475 - accuracy: 0.8066\n",
            "Epoch 180/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5481 - accuracy: 0.8064\n",
            "Epoch 181/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5430 - accuracy: 0.8060\n",
            "Epoch 182/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5373 - accuracy: 0.8093\n",
            "Epoch 183/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5318 - accuracy: 0.8103\n",
            "Epoch 184/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5324 - accuracy: 0.8096\n",
            "Epoch 185/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5416 - accuracy: 0.8071\n",
            "Epoch 186/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5307 - accuracy: 0.8134\n",
            "Epoch 187/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5346 - accuracy: 0.8095\n",
            "Epoch 188/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5232 - accuracy: 0.8146\n",
            "Epoch 189/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5231 - accuracy: 0.8148\n",
            "Epoch 190/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5246 - accuracy: 0.8133\n",
            "Epoch 191/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5216 - accuracy: 0.8162\n",
            "Epoch 192/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5291 - accuracy: 0.8121\n",
            "Epoch 193/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5266 - accuracy: 0.8131\n",
            "Epoch 194/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5171 - accuracy: 0.8172\n",
            "Epoch 195/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5256 - accuracy: 0.8149\n",
            "Epoch 196/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5256 - accuracy: 0.8125\n",
            "Epoch 197/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5229 - accuracy: 0.8146\n",
            "Epoch 198/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5197 - accuracy: 0.8181\n",
            "Epoch 199/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5190 - accuracy: 0.8148\n",
            "Epoch 200/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5299 - accuracy: 0.8132\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f8d3f6fc710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSgAzPkHE0QR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7cbb7c1-8572-4fde-bf62-a1d613cd1945"
      },
      "source": [
        "model.save_weights(\"/content/drive/My Drive/data_preprocessed_python/modelweight_valence_new.pkl\")\n",
        "model.save(\"/content/drive/My Drive/data_preprocessed_python/modeel_valence.pkl\")\n",
        "score = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/data_preprocessed_python/modeel_valence.pkl/assets\n",
            "229/229 [==============================] - 1s 3ms/step - loss: 0.5423 - accuracy: 0.8219\n",
            "Test loss: 0.5422682762145996\n",
            "Test accuracy: 0.8218579292297363\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hR-wB-NHE0QV"
      },
      "source": [
        "with open('/content/drive/My Drive/data_preprocessed_python/data_training.npy', 'rb') as fileTrain:\n",
        "    X  = np.load(fileTrain)\n",
        "    \n",
        "with open('/content/drive/My Drive/data_preprocessed_python/label_training.npy', 'rb') as fileTrainL:\n",
        "    Y  = np.load(fileTrainL)\n",
        "    \n",
        "X = normalize(X)\n",
        "Q = np.ravel(Y[:, [0]])\n",
        "\n",
        "Arousal_Train = np.ravel(Y[:, [0]])\n",
        "Valence_Train = np.ravel(Y[:, [1]])\n",
        "Domain_Train = np.ravel(Y[:, [2]])\n",
        "Like_Train = np.ravel(Y[:, [3]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0JymqNYOQMn"
      },
      "source": [
        "import pandas as pd\n",
        "import keras.backend as K\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.models import Sequential\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.utils import to_categorical \n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "import timeit\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Flatten, Dense, Dropout\n",
        "from keras.layers.convolutional import Convolution1D, MaxPooling1D, ZeroPadding1D\n",
        "from keras.optimizers import SGD\n",
        "#import cv2, numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXyPmRBLOW_9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b808dcff-3a50-4969-d9c1-7b0686c8bab8"
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "y_train = to_categorical(Q)\n",
        "y_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 1., 0., 0.],\n",
              "       [0., 0., 0., ..., 1., 0., 0.],\n",
              "       [0., 0., 0., ..., 1., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkNIG5tIOfA1"
      },
      "source": [
        "x_train = np.array(X[:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcmnDM4UOkiG"
      },
      "source": [
        "with open('/content/drive/My Drive/data_preprocessed_python/data_testing.npy', 'rb') as fileTrain:\n",
        "    M  = np.load(fileTrain)\n",
        "    \n",
        "with open('/content/drive/My Drive/data_preprocessed_python/label_testing.npy', 'rb') as fileTrainL:\n",
        "    N  = np.load(fileTrainL)\n",
        "\n",
        "M = normalize(M)\n",
        "W = np.ravel(N[:, [0]])\n",
        "\n",
        "Arousal_Test = np.ravel(N[:, [0]])\n",
        "Valence_Test = np.ravel(N[:, [1]])\n",
        "Domain_Test = np.ravel(N[:, [2]])\n",
        "Like_Test = np.ravel(N[:, [3]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4T6xwKyFOlkK"
      },
      "source": [
        "x_test = np.array(M[:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "batD0-nmOorZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68f7b948-6d06-472c-91b3-1121d9b904ea"
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "y_test = to_categorical(W)\n",
        "y_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 1., 0., 0.],\n",
              "       [0., 0., 0., ..., 1., 0., 0.],\n",
              "       [0., 0., 0., ..., 1., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svxKJPekOw8e"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_test = scaler.fit_transform(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnbFlwmROzoC"
      },
      "source": [
        "x_train = x_train.reshape(x_train.shape[0],x_train.shape[1], 1)\n",
        "x_test = x_test.reshape(x_test.shape[0],x_test.shape[1], 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewgu51F7O0Ud",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4bc9058-40db-4f3b-a3ec-7c6e7d5628b2"
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(51240, 70, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1U3FziSO6wv"
      },
      "source": [
        "batch_size = 256\n",
        "num_classes = 10\n",
        "epochs = 200\n",
        "input_shape=(x_train.shape[1], 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cz7ym1XyO9sk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "224bf568-3164-4c95-ae87-0aae329ac07d"
      },
      "source": [
        "print(input_shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(70, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ox4FVuG4O-VE"
      },
      "source": [
        "from keras.layers import Convolution1D, ZeroPadding1D, MaxPooling1D, BatchNormalization, Activation, Dropout, Flatten, Dense\n",
        "from keras.regularizers import l2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wz1wT8zfPBIb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e37412da-577b-47d9-f427-0dbd9cab1395"
      },
      "source": [
        "model = Sequential()\n",
        "intput_shape=(x_train.shape[1], 1)\n",
        "model.add(Conv1D(128, kernel_size=3,padding = 'same',activation='relu', input_shape=input_shape))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling1D(pool_size=(2)))\n",
        "model.add(Conv1D(128,kernel_size=3,padding = 'same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling1D(pool_size=(2)))\n",
        "#model.add(Conv1D(64,kernel_size=3,padding = 'same', activation='relu'))\n",
        "#model.add(MaxPooling1D(pool_size=(2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='tanh'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(32, activation='tanh'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_4 (Conv1D)            (None, 70, 128)           512       \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 70, 128)           512       \n",
            "_________________________________________________________________\n",
            "max_pooling1d_4 (MaxPooling1 (None, 35, 128)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_5 (Conv1D)            (None, 35, 128)           49280     \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 35, 128)           512       \n",
            "_________________________________________________________________\n",
            "max_pooling1d_5 (MaxPooling1 (None, 17, 128)           0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 2176)              0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 64)                139328    \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 10)                170       \n",
            "=================================================================\n",
            "Total params: 192,922\n",
            "Trainable params: 192,410\n",
            "Non-trainable params: 512\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pt77YCKkPHIQ"
      },
      "source": [
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-z28zEqPHA3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76fb5cd3-8ed7-4b3e-8fd4-608aacd5c380"
      },
      "source": [
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 1.9646 - accuracy: 0.2709\n",
            "Epoch 2/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 1.7820 - accuracy: 0.3311\n",
            "Epoch 3/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 1.6717 - accuracy: 0.3780\n",
            "Epoch 4/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 1.5888 - accuracy: 0.4083\n",
            "Epoch 5/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 1.5200 - accuracy: 0.4370\n",
            "Epoch 6/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 1.4564 - accuracy: 0.4637\n",
            "Epoch 7/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 1.3980 - accuracy: 0.4847\n",
            "Epoch 8/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 1.3467 - accuracy: 0.5073\n",
            "Epoch 9/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 1.3030 - accuracy: 0.5243\n",
            "Epoch 10/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 1.2589 - accuracy: 0.5405\n",
            "Epoch 11/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 1.2285 - accuracy: 0.5551\n",
            "Epoch 12/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 1.1929 - accuracy: 0.5677\n",
            "Epoch 13/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 1.1560 - accuracy: 0.5824\n",
            "Epoch 14/200\n",
            "201/201 [==============================] - 2s 7ms/step - loss: 1.1272 - accuracy: 0.5938\n",
            "Epoch 15/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 1.0914 - accuracy: 0.6056\n",
            "Epoch 16/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 1.0753 - accuracy: 0.6110\n",
            "Epoch 17/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 1.0543 - accuracy: 0.6224\n",
            "Epoch 18/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 1.0236 - accuracy: 0.6318\n",
            "Epoch 19/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 1.0057 - accuracy: 0.6398\n",
            "Epoch 20/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 0.9800 - accuracy: 0.6477\n",
            "Epoch 21/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 0.9794 - accuracy: 0.6510\n",
            "Epoch 22/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 0.9447 - accuracy: 0.6649\n",
            "Epoch 23/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 0.9306 - accuracy: 0.6670\n",
            "Epoch 24/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 0.9103 - accuracy: 0.6750\n",
            "Epoch 25/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 0.9032 - accuracy: 0.6777\n",
            "Epoch 26/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 0.8844 - accuracy: 0.6839\n",
            "Epoch 27/200\n",
            "201/201 [==============================] - 2s 7ms/step - loss: 0.8759 - accuracy: 0.6861\n",
            "Epoch 28/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 0.8613 - accuracy: 0.6925\n",
            "Epoch 29/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 0.8506 - accuracy: 0.6941\n",
            "Epoch 30/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 0.8392 - accuracy: 0.7002\n",
            "Epoch 31/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 0.8429 - accuracy: 0.6998\n",
            "Epoch 32/200\n",
            "201/201 [==============================] - 2s 7ms/step - loss: 0.8288 - accuracy: 0.7055\n",
            "Epoch 33/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 0.8231 - accuracy: 0.7069\n",
            "Epoch 34/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 0.8110 - accuracy: 0.7109\n",
            "Epoch 35/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 0.7999 - accuracy: 0.7154\n",
            "Epoch 36/200\n",
            "201/201 [==============================] - 2s 7ms/step - loss: 0.7931 - accuracy: 0.7173\n",
            "Epoch 37/200\n",
            "201/201 [==============================] - 2s 7ms/step - loss: 0.7865 - accuracy: 0.7196\n",
            "Epoch 38/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 0.7791 - accuracy: 0.7212\n",
            "Epoch 39/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 0.7706 - accuracy: 0.7234\n",
            "Epoch 40/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 0.7602 - accuracy: 0.7279\n",
            "Epoch 41/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 0.7629 - accuracy: 0.7290\n",
            "Epoch 42/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.7586 - accuracy: 0.7308\n",
            "Epoch 43/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 0.7544 - accuracy: 0.7321\n",
            "Epoch 44/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 0.7336 - accuracy: 0.7369\n",
            "Epoch 45/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 0.7363 - accuracy: 0.7377\n",
            "Epoch 46/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 0.7310 - accuracy: 0.7393\n",
            "Epoch 47/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 0.7345 - accuracy: 0.7384\n",
            "Epoch 48/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 0.7160 - accuracy: 0.7452\n",
            "Epoch 49/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 0.7149 - accuracy: 0.7469\n",
            "Epoch 50/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 0.7156 - accuracy: 0.7473\n",
            "Epoch 51/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 0.7043 - accuracy: 0.7509\n",
            "Epoch 52/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 0.7129 - accuracy: 0.7473\n",
            "Epoch 53/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 0.6921 - accuracy: 0.7548\n",
            "Epoch 54/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6966 - accuracy: 0.7541\n",
            "Epoch 55/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 0.6927 - accuracy: 0.7545\n",
            "Epoch 56/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 0.6781 - accuracy: 0.7605\n",
            "Epoch 57/200\n",
            "201/201 [==============================] - 2s 7ms/step - loss: 0.6747 - accuracy: 0.7590\n",
            "Epoch 58/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 0.6713 - accuracy: 0.7591\n",
            "Epoch 59/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6703 - accuracy: 0.7626\n",
            "Epoch 60/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 0.6729 - accuracy: 0.7615\n",
            "Epoch 61/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6666 - accuracy: 0.7645\n",
            "Epoch 62/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6655 - accuracy: 0.7640\n",
            "Epoch 63/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6681 - accuracy: 0.7647\n",
            "Epoch 64/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 0.6508 - accuracy: 0.7681\n",
            "Epoch 65/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 0.6515 - accuracy: 0.7704\n",
            "Epoch 66/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 0.6374 - accuracy: 0.7733\n",
            "Epoch 67/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 0.6493 - accuracy: 0.7704\n",
            "Epoch 68/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6456 - accuracy: 0.7735\n",
            "Epoch 69/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6468 - accuracy: 0.7720\n",
            "Epoch 70/200\n",
            "201/201 [==============================] - 2s 7ms/step - loss: 0.6339 - accuracy: 0.7757\n",
            "Epoch 71/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6301 - accuracy: 0.7788\n",
            "Epoch 72/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 0.6238 - accuracy: 0.7770\n",
            "Epoch 73/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 0.6221 - accuracy: 0.7815\n",
            "Epoch 74/200\n",
            "201/201 [==============================] - 2s 7ms/step - loss: 0.6250 - accuracy: 0.7777\n",
            "Epoch 75/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 0.6164 - accuracy: 0.7793\n",
            "Epoch 76/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 0.6225 - accuracy: 0.7793\n",
            "Epoch 77/200\n",
            "201/201 [==============================] - 2s 7ms/step - loss: 0.6203 - accuracy: 0.7818\n",
            "Epoch 78/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 0.6099 - accuracy: 0.7834\n",
            "Epoch 79/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6062 - accuracy: 0.7858\n",
            "Epoch 80/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 0.6040 - accuracy: 0.7875\n",
            "Epoch 81/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 0.6038 - accuracy: 0.7876\n",
            "Epoch 82/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 0.6124 - accuracy: 0.7831\n",
            "Epoch 83/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6113 - accuracy: 0.7851\n",
            "Epoch 84/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5977 - accuracy: 0.7895\n",
            "Epoch 85/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 0.5936 - accuracy: 0.7911\n",
            "Epoch 86/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 0.5914 - accuracy: 0.7888\n",
            "Epoch 87/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 0.5982 - accuracy: 0.7895\n",
            "Epoch 88/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 0.5863 - accuracy: 0.7949\n",
            "Epoch 89/200\n",
            "201/201 [==============================] - 2s 7ms/step - loss: 0.5874 - accuracy: 0.7925\n",
            "Epoch 90/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5761 - accuracy: 0.7977\n",
            "Epoch 91/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 0.5813 - accuracy: 0.7938\n",
            "Epoch 92/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5855 - accuracy: 0.7955\n",
            "Epoch 93/200\n",
            "201/201 [==============================] - 2s 7ms/step - loss: 0.5871 - accuracy: 0.7924\n",
            "Epoch 94/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 0.5778 - accuracy: 0.7967\n",
            "Epoch 95/200\n",
            "201/201 [==============================] - 2s 7ms/step - loss: 0.5630 - accuracy: 0.8022\n",
            "Epoch 96/200\n",
            "201/201 [==============================] - 2s 7ms/step - loss: 0.5728 - accuracy: 0.7983\n",
            "Epoch 97/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 0.5702 - accuracy: 0.8014\n",
            "Epoch 98/200\n",
            "201/201 [==============================] - 2s 7ms/step - loss: 0.5678 - accuracy: 0.8004\n",
            "Epoch 99/200\n",
            "201/201 [==============================] - 2s 7ms/step - loss: 0.5653 - accuracy: 0.8041\n",
            "Epoch 100/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5595 - accuracy: 0.8016\n",
            "Epoch 101/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 0.5619 - accuracy: 0.8054\n",
            "Epoch 102/200\n",
            "201/201 [==============================] - 2s 7ms/step - loss: 0.5559 - accuracy: 0.8060\n",
            "Epoch 103/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 0.5597 - accuracy: 0.8034\n",
            "Epoch 104/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5577 - accuracy: 0.8050\n",
            "Epoch 105/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 0.5515 - accuracy: 0.8067\n",
            "Epoch 106/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 0.5499 - accuracy: 0.8096\n",
            "Epoch 107/200\n",
            "201/201 [==============================] - 2s 7ms/step - loss: 0.5460 - accuracy: 0.8085\n",
            "Epoch 108/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 0.5470 - accuracy: 0.8093\n",
            "Epoch 109/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 0.5490 - accuracy: 0.8062\n",
            "Epoch 110/200\n",
            "201/201 [==============================] - 2s 7ms/step - loss: 0.5471 - accuracy: 0.8095\n",
            "Epoch 111/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5389 - accuracy: 0.8118\n",
            "Epoch 112/200\n",
            "201/201 [==============================] - 2s 7ms/step - loss: 0.5378 - accuracy: 0.8140\n",
            "Epoch 113/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 0.5489 - accuracy: 0.8095\n",
            "Epoch 114/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5379 - accuracy: 0.8105\n",
            "Epoch 115/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5430 - accuracy: 0.8119\n",
            "Epoch 116/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 0.5264 - accuracy: 0.8170\n",
            "Epoch 117/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 0.5334 - accuracy: 0.8141\n",
            "Epoch 118/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5239 - accuracy: 0.8173\n",
            "Epoch 119/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 0.5300 - accuracy: 0.8161\n",
            "Epoch 120/200\n",
            "201/201 [==============================] - 2s 7ms/step - loss: 0.5297 - accuracy: 0.8150\n",
            "Epoch 121/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5267 - accuracy: 0.8161\n",
            "Epoch 122/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5160 - accuracy: 0.8203\n",
            "Epoch 123/200\n",
            "201/201 [==============================] - 2s 7ms/step - loss: 0.5231 - accuracy: 0.8179\n",
            "Epoch 124/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5232 - accuracy: 0.8175\n",
            "Epoch 125/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5165 - accuracy: 0.8198\n",
            "Epoch 126/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5153 - accuracy: 0.8216\n",
            "Epoch 127/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 0.5284 - accuracy: 0.8169\n",
            "Epoch 128/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5089 - accuracy: 0.8235\n",
            "Epoch 129/200\n",
            "201/201 [==============================] - 2s 7ms/step - loss: 0.5138 - accuracy: 0.8233\n",
            "Epoch 130/200\n",
            "201/201 [==============================] - 2s 7ms/step - loss: 0.5107 - accuracy: 0.8232\n",
            "Epoch 131/200\n",
            "201/201 [==============================] - 2s 7ms/step - loss: 0.5032 - accuracy: 0.8249\n",
            "Epoch 132/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5021 - accuracy: 0.8261\n",
            "Epoch 133/200\n",
            "201/201 [==============================] - 2s 7ms/step - loss: 0.5102 - accuracy: 0.8230\n",
            "Epoch 134/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 0.4975 - accuracy: 0.8270\n",
            "Epoch 135/200\n",
            "201/201 [==============================] - 2s 7ms/step - loss: 0.4962 - accuracy: 0.8270\n",
            "Epoch 136/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5022 - accuracy: 0.8286\n",
            "Epoch 137/200\n",
            "201/201 [==============================] - 1s 7ms/step - loss: 0.5097 - accuracy: 0.8246\n",
            "Epoch 138/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5010 - accuracy: 0.8267\n",
            "Epoch 139/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5051 - accuracy: 0.8255\n",
            "Epoch 140/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5017 - accuracy: 0.8271\n",
            "Epoch 141/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4942 - accuracy: 0.8292\n",
            "Epoch 142/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4919 - accuracy: 0.8299\n",
            "Epoch 143/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4954 - accuracy: 0.8287\n",
            "Epoch 144/200\n",
            "201/201 [==============================] - 2s 7ms/step - loss: 0.4950 - accuracy: 0.8287\n",
            "Epoch 145/200\n",
            "201/201 [==============================] - 2s 7ms/step - loss: 0.4938 - accuracy: 0.8290\n",
            "Epoch 146/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4969 - accuracy: 0.8288\n",
            "Epoch 147/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4930 - accuracy: 0.8292\n",
            "Epoch 148/200\n",
            "201/201 [==============================] - 2s 7ms/step - loss: 0.4820 - accuracy: 0.8334\n",
            "Epoch 149/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4881 - accuracy: 0.8323\n",
            "Epoch 150/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4876 - accuracy: 0.8311\n",
            "Epoch 151/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4801 - accuracy: 0.8339\n",
            "Epoch 152/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4814 - accuracy: 0.8353\n",
            "Epoch 153/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4767 - accuracy: 0.8357\n",
            "Epoch 154/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4798 - accuracy: 0.8351\n",
            "Epoch 155/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4766 - accuracy: 0.8363\n",
            "Epoch 156/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4712 - accuracy: 0.8365\n",
            "Epoch 157/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4734 - accuracy: 0.8357\n",
            "Epoch 158/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4686 - accuracy: 0.8400\n",
            "Epoch 159/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4695 - accuracy: 0.8379\n",
            "Epoch 160/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4697 - accuracy: 0.8389\n",
            "Epoch 161/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4628 - accuracy: 0.8410\n",
            "Epoch 162/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4685 - accuracy: 0.8390\n",
            "Epoch 163/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4716 - accuracy: 0.8375\n",
            "Epoch 164/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4634 - accuracy: 0.8404\n",
            "Epoch 165/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4606 - accuracy: 0.8413\n",
            "Epoch 166/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4678 - accuracy: 0.8404\n",
            "Epoch 167/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4578 - accuracy: 0.8423\n",
            "Epoch 168/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4717 - accuracy: 0.8375\n",
            "Epoch 169/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4591 - accuracy: 0.8411\n",
            "Epoch 170/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4590 - accuracy: 0.8427\n",
            "Epoch 171/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4557 - accuracy: 0.8425\n",
            "Epoch 172/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4481 - accuracy: 0.8451\n",
            "Epoch 173/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4478 - accuracy: 0.8467\n",
            "Epoch 174/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4508 - accuracy: 0.8441\n",
            "Epoch 175/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4558 - accuracy: 0.8428\n",
            "Epoch 176/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4554 - accuracy: 0.8449\n",
            "Epoch 177/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4466 - accuracy: 0.8471\n",
            "Epoch 178/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4448 - accuracy: 0.8472\n",
            "Epoch 179/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4511 - accuracy: 0.8446\n",
            "Epoch 180/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4478 - accuracy: 0.8455\n",
            "Epoch 181/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4364 - accuracy: 0.8506\n",
            "Epoch 182/200\n",
            "201/201 [==============================] - 2s 7ms/step - loss: 0.4446 - accuracy: 0.8501\n",
            "Epoch 183/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4391 - accuracy: 0.8496\n",
            "Epoch 184/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4333 - accuracy: 0.8505\n",
            "Epoch 185/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4428 - accuracy: 0.8475\n",
            "Epoch 186/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4382 - accuracy: 0.8494\n",
            "Epoch 187/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4384 - accuracy: 0.8492\n",
            "Epoch 188/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4358 - accuracy: 0.8507\n",
            "Epoch 189/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4336 - accuracy: 0.8513\n",
            "Epoch 190/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4340 - accuracy: 0.8505\n",
            "Epoch 191/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4326 - accuracy: 0.8516\n",
            "Epoch 192/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4313 - accuracy: 0.8513\n",
            "Epoch 193/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4267 - accuracy: 0.8551\n",
            "Epoch 194/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4285 - accuracy: 0.8529\n",
            "Epoch 195/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4319 - accuracy: 0.8537\n",
            "Epoch 196/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4290 - accuracy: 0.8525\n",
            "Epoch 197/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4286 - accuracy: 0.8527\n",
            "Epoch 198/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4244 - accuracy: 0.8537\n",
            "Epoch 199/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4241 - accuracy: 0.8554\n",
            "Epoch 200/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4231 - accuracy: 0.8556\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f43c6143e10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGApOEoQPNQM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6efa5883-5977-4a66-afc1-854253da70a9"
      },
      "source": [
        "model.save_weights(\"/content/drive/My Drive/data_preprocessed_python/modelweight_AROUSAL_new.h5\")\n",
        "model.save(\"/content/drive/My Drive/data_preprocessed_python/modeel_AROUSAL.h5\")\n",
        "score = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "229/229 [==============================] - 1s 2ms/step - loss: 0.4710 - accuracy: 0.8515\n",
            "Test loss: 0.47099992632865906\n",
            "Test accuracy: 0.8515027165412903\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JbNDPMYQ4I0"
      },
      "source": [
        "with open('/content/drive/My Drive/data_preprocessed_python/data_training.npy', 'rb') as fileTrain:\n",
        "    X  = np.load(fileTrain)\n",
        "    \n",
        "with open('/content/drive/My Drive/data_preprocessed_python/label_training.npy', 'rb') as fileTrainL:\n",
        "    Y  = np.load(fileTrainL)\n",
        "    \n",
        "X = normalize(X)\n",
        "T = np.ravel(Y[:, [2]])\n",
        "\n",
        "Arousal_Train = np.ravel(Y[:, [0]])\n",
        "Valence_Train = np.ravel(Y[:, [1]])\n",
        "Domain_Train = np.ravel(Y[:, [2]])\n",
        "Like_Train = np.ravel(Y[:, [3]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olJDZCFRXxWb"
      },
      "source": [
        "import pandas as pd\n",
        "import keras.backend as K\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.models import Sequential\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.utils import to_categorical \n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "import timeit\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Flatten, Dense, Dropout\n",
        "from keras.layers.convolutional import Convolution1D, MaxPooling1D, ZeroPadding1D\n",
        "from keras.optimizers import SGD\n",
        "#import cv2, numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c244NmkQYDwm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a38be807-6ea6-4e12-ecff-f8209bf95bd1"
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "y_train = to_categorical(T)\n",
        "y_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ugEGFRsYF3w"
      },
      "source": [
        "x_train = np.array(X[:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4w4ly16tYNxl"
      },
      "source": [
        "with open('/content/drive/My Drive/data_preprocessed_python/data_testing.npy', 'rb') as fileTrain:\n",
        "    M  = np.load(fileTrain)\n",
        "    \n",
        "with open('/content/drive/My Drive/data_preprocessed_python/label_testing.npy', 'rb') as fileTrainL:\n",
        "    N  = np.load(fileTrainL)\n",
        "\n",
        "M = normalize(M)\n",
        "E = np.ravel(N[:, [2]])\n",
        "\n",
        "Arousal_Test = np.ravel(N[:, [0]])\n",
        "Valence_Test = np.ravel(N[:, [1]])\n",
        "Domain_Test = np.ravel(N[:, [2]])\n",
        "Like_Test = np.ravel(N[:, [3]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXQKwNomYiDg"
      },
      "source": [
        "x_test = np.array(M[:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmdwezx5Ympz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0df752ae-25cc-4da2-ed8d-45ae6750b041"
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "y_test = to_categorical(E)\n",
        "y_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peCnhEymYvMC"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_test = scaler.fit_transform(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oms5ZT9RY3cA"
      },
      "source": [
        "x_train = x_train.reshape(x_train.shape[0],x_train.shape[1], 1)\n",
        "x_test = x_test.reshape(x_test.shape[0],x_test.shape[1], 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTh-T7WsY4M4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4baeb63-1902-4d87-8dc2-4dc7b74c7ed6"
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(51240, 70, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGUZtBijY9-8"
      },
      "source": [
        "batch_size = 256\n",
        "num_classes = 10\n",
        "epochs = 200\n",
        "input_shape=(x_train.shape[1], 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXPYo8trZBj0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee98c436-3760-4ada-fccb-685be70a6344"
      },
      "source": [
        "print(input_shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(70, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nD4PuRakZEmA"
      },
      "source": [
        "from keras.layers import Convolution1D, ZeroPadding1D, MaxPooling1D, BatchNormalization, Activation, Dropout, Flatten, Dense\n",
        "from keras.regularizers import l2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFJdw2PIZFGW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb8a6e7e-e96a-4d9f-f281-5f935db2ce6e"
      },
      "source": [
        "model = Sequential()\n",
        "intput_shape=(x_train.shape[1], 1)\n",
        "model.add(Conv1D(128, kernel_size=3,padding = 'same',activation='relu', input_shape=input_shape))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling1D(pool_size=(2)))\n",
        "model.add(Conv1D(128,kernel_size=3,padding = 'same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling1D(pool_size=(2)))\n",
        "#model.add(Conv1D(64,kernel_size=3,padding = 'same', activation='relu'))\n",
        "#model.add(MaxPooling1D(pool_size=(2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='tanh'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(32, activation='tanh'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d (Conv1D)              (None, 70, 128)           512       \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 70, 128)           512       \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, 35, 128)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 35, 128)           49280     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 35, 128)           512       \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 17, 128)           0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 2176)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                139328    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                170       \n",
            "=================================================================\n",
            "Total params: 192,922\n",
            "Trainable params: 192,410\n",
            "Non-trainable params: 512\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8YDc_CEZL6B"
      },
      "source": [
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEP96rV7ZPET",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "941e1e2d-5653-49ae-ac73-ddcb3ceffe15"
      },
      "source": [
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 1.8714 - accuracy: 0.2738\n",
            "Epoch 2/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 1.6843 - accuracy: 0.3317\n",
            "Epoch 3/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 1.5799 - accuracy: 0.3773\n",
            "Epoch 4/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 1.5094 - accuracy: 0.4097\n",
            "Epoch 5/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 1.4532 - accuracy: 0.4347\n",
            "Epoch 6/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 1.3975 - accuracy: 0.4592\n",
            "Epoch 7/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 1.3670 - accuracy: 0.4719\n",
            "Epoch 8/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 1.3187 - accuracy: 0.4941\n",
            "Epoch 9/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 1.2821 - accuracy: 0.5112\n",
            "Epoch 10/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 1.2452 - accuracy: 0.5265\n",
            "Epoch 11/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 1.2198 - accuracy: 0.5365\n",
            "Epoch 12/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 1.1936 - accuracy: 0.5495\n",
            "Epoch 13/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 1.1574 - accuracy: 0.5628\n",
            "Epoch 14/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 1.1456 - accuracy: 0.5689\n",
            "Epoch 15/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 1.1198 - accuracy: 0.5810\n",
            "Epoch 16/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 1.0926 - accuracy: 0.5897\n",
            "Epoch 17/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 1.0743 - accuracy: 0.5980\n",
            "Epoch 18/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 1.0657 - accuracy: 0.5967\n",
            "Epoch 19/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 1.0429 - accuracy: 0.6079\n",
            "Epoch 20/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 1.0198 - accuracy: 0.6183\n",
            "Epoch 21/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 1.0088 - accuracy: 0.6218\n",
            "Epoch 22/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.9896 - accuracy: 0.6308\n",
            "Epoch 23/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.9782 - accuracy: 0.6352\n",
            "Epoch 24/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.9693 - accuracy: 0.6378\n",
            "Epoch 25/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.9621 - accuracy: 0.6407\n",
            "Epoch 26/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.9390 - accuracy: 0.6517\n",
            "Epoch 27/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.9304 - accuracy: 0.6529\n",
            "Epoch 28/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.9176 - accuracy: 0.6612\n",
            "Epoch 29/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.9089 - accuracy: 0.6625\n",
            "Epoch 30/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.8989 - accuracy: 0.6687\n",
            "Epoch 31/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.8893 - accuracy: 0.6711\n",
            "Epoch 32/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.8783 - accuracy: 0.6741\n",
            "Epoch 33/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.8721 - accuracy: 0.6785\n",
            "Epoch 34/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.8620 - accuracy: 0.6806\n",
            "Epoch 35/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.8586 - accuracy: 0.6820\n",
            "Epoch 36/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.8451 - accuracy: 0.6906\n",
            "Epoch 37/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.8429 - accuracy: 0.6886\n",
            "Epoch 38/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.8439 - accuracy: 0.6888\n",
            "Epoch 39/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.8281 - accuracy: 0.6930\n",
            "Epoch 40/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.8180 - accuracy: 0.6977\n",
            "Epoch 41/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.8129 - accuracy: 0.7019\n",
            "Epoch 42/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.8115 - accuracy: 0.7016\n",
            "Epoch 43/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.7993 - accuracy: 0.7071\n",
            "Epoch 44/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.7938 - accuracy: 0.7088\n",
            "Epoch 45/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.7907 - accuracy: 0.7121\n",
            "Epoch 46/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.7792 - accuracy: 0.7156\n",
            "Epoch 47/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.7743 - accuracy: 0.7172\n",
            "Epoch 48/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.7753 - accuracy: 0.7154\n",
            "Epoch 49/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.7728 - accuracy: 0.7177\n",
            "Epoch 50/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.7683 - accuracy: 0.7200\n",
            "Epoch 51/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.7654 - accuracy: 0.7200\n",
            "Epoch 52/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.7503 - accuracy: 0.7253\n",
            "Epoch 53/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.7540 - accuracy: 0.7262\n",
            "Epoch 54/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.7488 - accuracy: 0.7263\n",
            "Epoch 55/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.7474 - accuracy: 0.7270\n",
            "Epoch 56/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.7432 - accuracy: 0.7300\n",
            "Epoch 57/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.7294 - accuracy: 0.7352\n",
            "Epoch 58/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.7340 - accuracy: 0.7326\n",
            "Epoch 59/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.7288 - accuracy: 0.7351\n",
            "Epoch 60/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.7198 - accuracy: 0.7381\n",
            "Epoch 61/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.7206 - accuracy: 0.7385\n",
            "Epoch 62/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.7102 - accuracy: 0.7419\n",
            "Epoch 63/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.7185 - accuracy: 0.7369\n",
            "Epoch 64/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.7200 - accuracy: 0.7377\n",
            "Epoch 65/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.7018 - accuracy: 0.7442\n",
            "Epoch 66/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.7023 - accuracy: 0.7428\n",
            "Epoch 67/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6988 - accuracy: 0.7457\n",
            "Epoch 68/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6912 - accuracy: 0.7480\n",
            "Epoch 69/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6925 - accuracy: 0.7484\n",
            "Epoch 70/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6957 - accuracy: 0.7467\n",
            "Epoch 71/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6864 - accuracy: 0.7521\n",
            "Epoch 72/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6788 - accuracy: 0.7543\n",
            "Epoch 73/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6752 - accuracy: 0.7556\n",
            "Epoch 74/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6720 - accuracy: 0.7570\n",
            "Epoch 75/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6841 - accuracy: 0.7529\n",
            "Epoch 76/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6709 - accuracy: 0.7575\n",
            "Epoch 77/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6688 - accuracy: 0.7588\n",
            "Epoch 78/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6561 - accuracy: 0.7637\n",
            "Epoch 79/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6571 - accuracy: 0.7630\n",
            "Epoch 80/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6598 - accuracy: 0.7601\n",
            "Epoch 81/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6691 - accuracy: 0.7601\n",
            "Epoch 82/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6736 - accuracy: 0.7558\n",
            "Epoch 83/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6523 - accuracy: 0.7639\n",
            "Epoch 84/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6543 - accuracy: 0.7624\n",
            "Epoch 85/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6523 - accuracy: 0.7650\n",
            "Epoch 86/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6468 - accuracy: 0.7678\n",
            "Epoch 87/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6440 - accuracy: 0.7683\n",
            "Epoch 88/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6396 - accuracy: 0.7701\n",
            "Epoch 89/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6393 - accuracy: 0.7698\n",
            "Epoch 90/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6388 - accuracy: 0.7688\n",
            "Epoch 91/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6372 - accuracy: 0.7711\n",
            "Epoch 92/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6291 - accuracy: 0.7728\n",
            "Epoch 93/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6263 - accuracy: 0.7752\n",
            "Epoch 94/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6239 - accuracy: 0.7755\n",
            "Epoch 95/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6285 - accuracy: 0.7728\n",
            "Epoch 96/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6122 - accuracy: 0.7788\n",
            "Epoch 97/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6235 - accuracy: 0.7754\n",
            "Epoch 98/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6178 - accuracy: 0.7749\n",
            "Epoch 99/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6222 - accuracy: 0.7756\n",
            "Epoch 100/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6160 - accuracy: 0.7792\n",
            "Epoch 101/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6177 - accuracy: 0.7781\n",
            "Epoch 102/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6014 - accuracy: 0.7834\n",
            "Epoch 103/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6115 - accuracy: 0.7794\n",
            "Epoch 104/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6107 - accuracy: 0.7827\n",
            "Epoch 105/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5989 - accuracy: 0.7821\n",
            "Epoch 106/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6055 - accuracy: 0.7825\n",
            "Epoch 107/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5947 - accuracy: 0.7847\n",
            "Epoch 108/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6001 - accuracy: 0.7840\n",
            "Epoch 109/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6014 - accuracy: 0.7841\n",
            "Epoch 110/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5886 - accuracy: 0.7901\n",
            "Epoch 111/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5928 - accuracy: 0.7865\n",
            "Epoch 112/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5993 - accuracy: 0.7843\n",
            "Epoch 113/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5943 - accuracy: 0.7858\n",
            "Epoch 114/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5893 - accuracy: 0.7868\n",
            "Epoch 115/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5828 - accuracy: 0.7905\n",
            "Epoch 116/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5850 - accuracy: 0.7906\n",
            "Epoch 117/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5862 - accuracy: 0.7896\n",
            "Epoch 118/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5782 - accuracy: 0.7933\n",
            "Epoch 119/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5781 - accuracy: 0.7932\n",
            "Epoch 120/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5757 - accuracy: 0.7940\n",
            "Epoch 121/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5769 - accuracy: 0.7930\n",
            "Epoch 122/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5713 - accuracy: 0.7959\n",
            "Epoch 123/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5725 - accuracy: 0.7949\n",
            "Epoch 124/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5735 - accuracy: 0.7948\n",
            "Epoch 125/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5642 - accuracy: 0.7970\n",
            "Epoch 126/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5648 - accuracy: 0.7984\n",
            "Epoch 127/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5655 - accuracy: 0.7969\n",
            "Epoch 128/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5611 - accuracy: 0.7982\n",
            "Epoch 129/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5593 - accuracy: 0.8006\n",
            "Epoch 130/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5550 - accuracy: 0.8005\n",
            "Epoch 131/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5600 - accuracy: 0.7996\n",
            "Epoch 132/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5586 - accuracy: 0.8000\n",
            "Epoch 133/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5547 - accuracy: 0.8014\n",
            "Epoch 134/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5515 - accuracy: 0.8026\n",
            "Epoch 135/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5525 - accuracy: 0.8037\n",
            "Epoch 136/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5464 - accuracy: 0.8022\n",
            "Epoch 137/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5441 - accuracy: 0.8044\n",
            "Epoch 138/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5526 - accuracy: 0.8014\n",
            "Epoch 139/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5398 - accuracy: 0.8071\n",
            "Epoch 140/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5572 - accuracy: 0.8004\n",
            "Epoch 141/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5404 - accuracy: 0.8071\n",
            "Epoch 142/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5418 - accuracy: 0.8058\n",
            "Epoch 143/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5439 - accuracy: 0.8044\n",
            "Epoch 144/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5436 - accuracy: 0.8049\n",
            "Epoch 145/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5374 - accuracy: 0.8081\n",
            "Epoch 146/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5420 - accuracy: 0.8068\n",
            "Epoch 147/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5435 - accuracy: 0.8074\n",
            "Epoch 148/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5330 - accuracy: 0.8080\n",
            "Epoch 149/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5323 - accuracy: 0.8090\n",
            "Epoch 150/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5374 - accuracy: 0.8081\n",
            "Epoch 151/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5257 - accuracy: 0.8101\n",
            "Epoch 152/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5268 - accuracy: 0.8137\n",
            "Epoch 153/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5265 - accuracy: 0.8102\n",
            "Epoch 154/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5233 - accuracy: 0.8144\n",
            "Epoch 155/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5292 - accuracy: 0.8121\n",
            "Epoch 156/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5228 - accuracy: 0.8138\n",
            "Epoch 157/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5214 - accuracy: 0.8154\n",
            "Epoch 158/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5179 - accuracy: 0.8144\n",
            "Epoch 159/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5169 - accuracy: 0.8160\n",
            "Epoch 160/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5181 - accuracy: 0.8159\n",
            "Epoch 161/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5108 - accuracy: 0.8179\n",
            "Epoch 162/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5124 - accuracy: 0.8189\n",
            "Epoch 163/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5142 - accuracy: 0.8173\n",
            "Epoch 164/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5082 - accuracy: 0.8191\n",
            "Epoch 165/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5205 - accuracy: 0.8144\n",
            "Epoch 166/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5045 - accuracy: 0.8192\n",
            "Epoch 167/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5187 - accuracy: 0.8158\n",
            "Epoch 168/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5167 - accuracy: 0.8158\n",
            "Epoch 169/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5078 - accuracy: 0.8189\n",
            "Epoch 170/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5098 - accuracy: 0.8180\n",
            "Epoch 171/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5036 - accuracy: 0.8208\n",
            "Epoch 172/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5008 - accuracy: 0.8228\n",
            "Epoch 173/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5062 - accuracy: 0.8192\n",
            "Epoch 174/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5021 - accuracy: 0.8215\n",
            "Epoch 175/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4996 - accuracy: 0.8226\n",
            "Epoch 176/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5053 - accuracy: 0.8206\n",
            "Epoch 177/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5031 - accuracy: 0.8198\n",
            "Epoch 178/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4902 - accuracy: 0.8252\n",
            "Epoch 179/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4915 - accuracy: 0.8256\n",
            "Epoch 180/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4878 - accuracy: 0.8259\n",
            "Epoch 181/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4982 - accuracy: 0.8233\n",
            "Epoch 182/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4925 - accuracy: 0.8258\n",
            "Epoch 183/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4905 - accuracy: 0.8255\n",
            "Epoch 184/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4869 - accuracy: 0.8275\n",
            "Epoch 185/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4872 - accuracy: 0.8271\n",
            "Epoch 186/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4880 - accuracy: 0.8263\n",
            "Epoch 187/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4910 - accuracy: 0.8259\n",
            "Epoch 188/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4890 - accuracy: 0.8267\n",
            "Epoch 189/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4847 - accuracy: 0.8283\n",
            "Epoch 190/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4806 - accuracy: 0.8285\n",
            "Epoch 191/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4890 - accuracy: 0.8258\n",
            "Epoch 192/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4871 - accuracy: 0.8276\n",
            "Epoch 193/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4784 - accuracy: 0.8299\n",
            "Epoch 194/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4835 - accuracy: 0.8278\n",
            "Epoch 195/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4842 - accuracy: 0.8286\n",
            "Epoch 196/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4762 - accuracy: 0.8311\n",
            "Epoch 197/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4837 - accuracy: 0.8286\n",
            "Epoch 198/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4720 - accuracy: 0.8327\n",
            "Epoch 199/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4693 - accuracy: 0.8347\n",
            "Epoch 200/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4821 - accuracy: 0.8289\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ff75c3faef0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-qIH34JZPn4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40ade604-620d-4f96-97bd-6f19c83dcfc0"
      },
      "source": [
        "model.save_weights(\"/content/drive/My Drive/data_preprocessed_python/modelweight_dominance_new.h5\")\n",
        "model.save(\"/content/drive/My Drive/data_preprocessed_python/modeel_dominance.h5\")\n",
        "score = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "229/229 [==============================] - 1s 3ms/step - loss: 0.5364 - accuracy: 0.8186\n",
            "Test loss: 0.5364093780517578\n",
            "Test accuracy: 0.8185792565345764\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkH1D-tiewLA"
      },
      "source": [
        "with open('/content/drive/My Drive/data_preprocessed_python/data_training.npy', 'rb') as fileTrain:\n",
        "    X  = np.load(fileTrain)\n",
        "    \n",
        "with open('/content/drive/My Drive/data_preprocessed_python/label_training.npy', 'rb') as fileTrainL:\n",
        "    Y  = np.load(fileTrainL)\n",
        "    \n",
        "X = normalize(X)\n",
        "R = np.ravel(Y[:, [3]])\n",
        "\n",
        "Arousal_Train = np.ravel(Y[:, [0]])\n",
        "Valence_Train = np.ravel(Y[:, [1]])\n",
        "Domain_Train = np.ravel(Y[:, [2]])\n",
        "Like_Train = np.ravel(Y[:, [3]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPjst-bhfIdt"
      },
      "source": [
        "import pandas as pd\n",
        "import keras.backend as K\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.models import Sequential\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.utils import to_categorical \n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "import timeit\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Flatten, Dense, Dropout\n",
        "from keras.layers.convolutional import Convolution1D, MaxPooling1D, ZeroPadding1D\n",
        "from keras.optimizers import SGD\n",
        "#import cv2, numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e92XJnPLfM39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07c6ac76-e835-4b6e-cc73-fdec155fe603"
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "y_train = to_categorical(R)\n",
        "y_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 1., 0., 0.],\n",
              "       [0., 0., 0., ..., 1., 0., 0.],\n",
              "       [0., 0., 0., ..., 1., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jU2gQDifNuT"
      },
      "source": [
        "x_train = np.array(X[:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Bo-UdBKfbIQ"
      },
      "source": [
        "with open('/content/drive/My Drive/data_preprocessed_python/data_testing.npy', 'rb') as fileTrain:\n",
        "    M  = np.load(fileTrain)\n",
        "    \n",
        "with open('/content/drive/My Drive/data_preprocessed_python/label_testing.npy', 'rb') as fileTrainL:\n",
        "    N  = np.load(fileTrainL)\n",
        "\n",
        "M = normalize(M)\n",
        "F = np.ravel(N[:, [3]])\n",
        "\n",
        "Arousal_Test = np.ravel(N[:, [0]])\n",
        "Valence_Test = np.ravel(N[:, [1]])\n",
        "Domain_Test = np.ravel(N[:, [2]])\n",
        "Like_Test = np.ravel(N[:, [3]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPjlJujYfm5s"
      },
      "source": [
        "x_test = np.array(M[:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDVD1UO_fqb4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b19f7db1-af75-4317-ad90-b5aae7be48fb"
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "y_test = to_categorical(F)\n",
        "y_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 1., 0., 0.],\n",
              "       [0., 0., 0., ..., 1., 0., 0.],\n",
              "       [0., 0., 0., ..., 1., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dd0zbm71fqap"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_test = scaler.fit_transform(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6FKEigkfurw"
      },
      "source": [
        "x_train = x_train.reshape(x_train.shape[0],x_train.shape[1], 1)\n",
        "x_test = x_test.reshape(x_test.shape[0],x_test.shape[1], 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2IBvrX7fyFU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f1f4d68-5d16-43b9-81e7-c4e35e4450c2"
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(51240, 70, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVA0AIymf4u4"
      },
      "source": [
        "batch_size = 256\n",
        "num_classes = 10\n",
        "epochs = 200\n",
        "input_shape=(x_train.shape[1], 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFA36k6nf_DN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad17e978-c737-4775-d156-dca61a55e8e9"
      },
      "source": [
        "print(input_shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(70, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QIrDLuYf_x9"
      },
      "source": [
        "from keras.layers import Convolution1D, ZeroPadding1D, MaxPooling1D, BatchNormalization, Activation, Dropout, Flatten, Dense\n",
        "from keras.regularizers import l2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_lvx0K3gF5X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70e96a73-cddc-4ffb-cedf-4fca78414a85"
      },
      "source": [
        "model = Sequential()\n",
        "intput_shape=(x_train.shape[1], 1)\n",
        "model.add(Conv1D(128, kernel_size=3,padding = 'same',activation='relu', input_shape=input_shape))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling1D(pool_size=(2)))\n",
        "model.add(Conv1D(128,kernel_size=3,padding = 'same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling1D(pool_size=(2)))\n",
        "#model.add(Conv1D(64,kernel_size=3,padding = 'same', activation='relu'))\n",
        "#model.add(MaxPooling1D(pool_size=(2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='tanh'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(32, activation='tanh'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_2 (Conv1D)            (None, 70, 128)           512       \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 70, 128)           512       \n",
            "_________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1 (None, 35, 128)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_3 (Conv1D)            (None, 35, 128)           49280     \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 35, 128)           512       \n",
            "_________________________________________________________________\n",
            "max_pooling1d_3 (MaxPooling1 (None, 17, 128)           0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 2176)              0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 64)                139328    \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 10)                170       \n",
            "=================================================================\n",
            "Total params: 192,922\n",
            "Trainable params: 192,410\n",
            "Non-trainable params: 512\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bl1L3Ex9gJJ-"
      },
      "source": [
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwxTRpY0gNG5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8de23386-f137-4019-fc76-b22b2720917c"
      },
      "source": [
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 1.6896 - accuracy: 0.3181\n",
            "Epoch 2/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 1.4827 - accuracy: 0.3787\n",
            "Epoch 3/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 1.3870 - accuracy: 0.4210\n",
            "Epoch 4/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 1.3026 - accuracy: 0.4605\n",
            "Epoch 5/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 1.2339 - accuracy: 0.4940\n",
            "Epoch 6/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 1.1786 - accuracy: 0.5210\n",
            "Epoch 7/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 1.1274 - accuracy: 0.5414\n",
            "Epoch 8/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 1.0801 - accuracy: 0.5597\n",
            "Epoch 9/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 1.0468 - accuracy: 0.5767\n",
            "Epoch 10/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 1.0054 - accuracy: 0.5927\n",
            "Epoch 11/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.9836 - accuracy: 0.6034\n",
            "Epoch 12/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.9470 - accuracy: 0.6209\n",
            "Epoch 13/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.9254 - accuracy: 0.6303\n",
            "Epoch 14/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.9055 - accuracy: 0.6381\n",
            "Epoch 15/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.8828 - accuracy: 0.6487\n",
            "Epoch 16/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.8630 - accuracy: 0.6556\n",
            "Epoch 17/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.8454 - accuracy: 0.6605\n",
            "Epoch 18/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.8336 - accuracy: 0.6698\n",
            "Epoch 19/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.8113 - accuracy: 0.6747\n",
            "Epoch 20/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.7996 - accuracy: 0.6818\n",
            "Epoch 21/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.7919 - accuracy: 0.6844\n",
            "Epoch 22/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.7766 - accuracy: 0.6934\n",
            "Epoch 23/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.7662 - accuracy: 0.6959\n",
            "Epoch 24/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.7537 - accuracy: 0.7011\n",
            "Epoch 25/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.7484 - accuracy: 0.7037\n",
            "Epoch 26/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.7391 - accuracy: 0.7056\n",
            "Epoch 27/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.7188 - accuracy: 0.7145\n",
            "Epoch 28/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.7163 - accuracy: 0.7161\n",
            "Epoch 29/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.7107 - accuracy: 0.7195\n",
            "Epoch 30/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6999 - accuracy: 0.7240\n",
            "Epoch 31/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6850 - accuracy: 0.7279\n",
            "Epoch 32/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6859 - accuracy: 0.7291\n",
            "Epoch 33/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6783 - accuracy: 0.7326\n",
            "Epoch 34/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6739 - accuracy: 0.7359\n",
            "Epoch 35/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6619 - accuracy: 0.7361\n",
            "Epoch 36/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6512 - accuracy: 0.7414\n",
            "Epoch 37/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6526 - accuracy: 0.7435\n",
            "Epoch 38/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6489 - accuracy: 0.7464\n",
            "Epoch 39/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6467 - accuracy: 0.7462\n",
            "Epoch 40/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6470 - accuracy: 0.7469\n",
            "Epoch 41/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6230 - accuracy: 0.7523\n",
            "Epoch 42/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6208 - accuracy: 0.7552\n",
            "Epoch 43/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6201 - accuracy: 0.7545\n",
            "Epoch 44/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6212 - accuracy: 0.7558\n",
            "Epoch 45/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6085 - accuracy: 0.7612\n",
            "Epoch 46/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.6001 - accuracy: 0.7631\n",
            "Epoch 47/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5999 - accuracy: 0.7636\n",
            "Epoch 48/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5992 - accuracy: 0.7643\n",
            "Epoch 49/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5942 - accuracy: 0.7645\n",
            "Epoch 50/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5880 - accuracy: 0.7691\n",
            "Epoch 51/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5792 - accuracy: 0.7713\n",
            "Epoch 52/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5955 - accuracy: 0.7650\n",
            "Epoch 53/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5736 - accuracy: 0.7760\n",
            "Epoch 54/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5754 - accuracy: 0.7728\n",
            "Epoch 55/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5698 - accuracy: 0.7762\n",
            "Epoch 56/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5669 - accuracy: 0.7754\n",
            "Epoch 57/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5600 - accuracy: 0.7789\n",
            "Epoch 58/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5634 - accuracy: 0.7802\n",
            "Epoch 59/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5507 - accuracy: 0.7853\n",
            "Epoch 60/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5501 - accuracy: 0.7828\n",
            "Epoch 61/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5442 - accuracy: 0.7868\n",
            "Epoch 62/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5502 - accuracy: 0.7846\n",
            "Epoch 63/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5357 - accuracy: 0.7898\n",
            "Epoch 64/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5318 - accuracy: 0.7917\n",
            "Epoch 65/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5296 - accuracy: 0.7942\n",
            "Epoch 66/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5330 - accuracy: 0.7902\n",
            "Epoch 67/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5337 - accuracy: 0.7924\n",
            "Epoch 68/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5297 - accuracy: 0.7920\n",
            "Epoch 69/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5311 - accuracy: 0.7921\n",
            "Epoch 70/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5249 - accuracy: 0.7959\n",
            "Epoch 71/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5289 - accuracy: 0.7929\n",
            "Epoch 72/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5165 - accuracy: 0.7982\n",
            "Epoch 73/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5224 - accuracy: 0.7949\n",
            "Epoch 74/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5160 - accuracy: 0.7985\n",
            "Epoch 75/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5203 - accuracy: 0.7989\n",
            "Epoch 76/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5098 - accuracy: 0.8002\n",
            "Epoch 77/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5053 - accuracy: 0.8031\n",
            "Epoch 78/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5068 - accuracy: 0.8010\n",
            "Epoch 79/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5045 - accuracy: 0.8048\n",
            "Epoch 80/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5035 - accuracy: 0.8034\n",
            "Epoch 81/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4943 - accuracy: 0.8063\n",
            "Epoch 82/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4975 - accuracy: 0.8084\n",
            "Epoch 83/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4924 - accuracy: 0.8098\n",
            "Epoch 84/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5136 - accuracy: 0.8002\n",
            "Epoch 85/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4920 - accuracy: 0.8089\n",
            "Epoch 86/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4914 - accuracy: 0.8075\n",
            "Epoch 87/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4900 - accuracy: 0.8103\n",
            "Epoch 88/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4812 - accuracy: 0.8123\n",
            "Epoch 89/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.5065 - accuracy: 0.8059\n",
            "Epoch 90/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4750 - accuracy: 0.8165\n",
            "Epoch 91/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4841 - accuracy: 0.8121\n",
            "Epoch 92/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4757 - accuracy: 0.8149\n",
            "Epoch 93/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4822 - accuracy: 0.8125\n",
            "Epoch 94/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4685 - accuracy: 0.8177\n",
            "Epoch 95/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4690 - accuracy: 0.8185\n",
            "Epoch 96/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4675 - accuracy: 0.8194\n",
            "Epoch 97/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4564 - accuracy: 0.8222\n",
            "Epoch 98/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4681 - accuracy: 0.8200\n",
            "Epoch 99/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4620 - accuracy: 0.8214\n",
            "Epoch 100/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4634 - accuracy: 0.8215\n",
            "Epoch 101/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4623 - accuracy: 0.8230\n",
            "Epoch 102/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4572 - accuracy: 0.8256\n",
            "Epoch 103/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4477 - accuracy: 0.8264\n",
            "Epoch 104/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4575 - accuracy: 0.8217\n",
            "Epoch 105/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4617 - accuracy: 0.8229\n",
            "Epoch 106/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4546 - accuracy: 0.8234\n",
            "Epoch 107/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4532 - accuracy: 0.8259\n",
            "Epoch 108/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4593 - accuracy: 0.8242\n",
            "Epoch 109/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4481 - accuracy: 0.8257\n",
            "Epoch 110/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4460 - accuracy: 0.8282\n",
            "Epoch 111/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4477 - accuracy: 0.8296\n",
            "Epoch 112/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4407 - accuracy: 0.8305\n",
            "Epoch 113/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4383 - accuracy: 0.8326\n",
            "Epoch 114/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4315 - accuracy: 0.8339\n",
            "Epoch 115/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4330 - accuracy: 0.8338\n",
            "Epoch 116/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4286 - accuracy: 0.8351\n",
            "Epoch 117/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4211 - accuracy: 0.8372\n",
            "Epoch 118/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4338 - accuracy: 0.8356\n",
            "Epoch 119/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4287 - accuracy: 0.8347\n",
            "Epoch 120/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4331 - accuracy: 0.8349\n",
            "Epoch 121/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4245 - accuracy: 0.8363\n",
            "Epoch 122/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4287 - accuracy: 0.8355\n",
            "Epoch 123/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4215 - accuracy: 0.8385\n",
            "Epoch 124/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4221 - accuracy: 0.8382\n",
            "Epoch 125/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4261 - accuracy: 0.8354\n",
            "Epoch 126/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4239 - accuracy: 0.8389\n",
            "Epoch 127/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4128 - accuracy: 0.8435\n",
            "Epoch 128/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4150 - accuracy: 0.8425\n",
            "Epoch 129/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4126 - accuracy: 0.8430\n",
            "Epoch 130/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4151 - accuracy: 0.8425\n",
            "Epoch 131/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4171 - accuracy: 0.8430\n",
            "Epoch 132/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4047 - accuracy: 0.8453\n",
            "Epoch 133/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.3982 - accuracy: 0.8489\n",
            "Epoch 134/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4120 - accuracy: 0.8460\n",
            "Epoch 135/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4063 - accuracy: 0.8465\n",
            "Epoch 136/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4017 - accuracy: 0.8481\n",
            "Epoch 137/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4060 - accuracy: 0.8473\n",
            "Epoch 138/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4078 - accuracy: 0.8459\n",
            "Epoch 139/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4023 - accuracy: 0.8472\n",
            "Epoch 140/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.3937 - accuracy: 0.8511\n",
            "Epoch 141/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.3940 - accuracy: 0.8506\n",
            "Epoch 142/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.3990 - accuracy: 0.8508\n",
            "Epoch 143/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.4074 - accuracy: 0.8448\n",
            "Epoch 144/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.3946 - accuracy: 0.8509\n",
            "Epoch 145/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.3979 - accuracy: 0.8491\n",
            "Epoch 146/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.3868 - accuracy: 0.8537\n",
            "Epoch 147/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.3919 - accuracy: 0.8514\n",
            "Epoch 148/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.3885 - accuracy: 0.8530\n",
            "Epoch 149/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.3931 - accuracy: 0.8529\n",
            "Epoch 150/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.3872 - accuracy: 0.8540\n",
            "Epoch 151/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.3877 - accuracy: 0.8542\n",
            "Epoch 152/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.3843 - accuracy: 0.8563\n",
            "Epoch 153/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.3777 - accuracy: 0.8589\n",
            "Epoch 154/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.3779 - accuracy: 0.8574\n",
            "Epoch 155/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.3745 - accuracy: 0.8587\n",
            "Epoch 156/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.3731 - accuracy: 0.8597\n",
            "Epoch 157/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.3758 - accuracy: 0.8567\n",
            "Epoch 158/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.3795 - accuracy: 0.8577\n",
            "Epoch 159/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.3800 - accuracy: 0.8577\n",
            "Epoch 160/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.3712 - accuracy: 0.8605\n",
            "Epoch 161/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.3666 - accuracy: 0.8618\n",
            "Epoch 162/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.3750 - accuracy: 0.8613\n",
            "Epoch 163/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.3749 - accuracy: 0.8579\n",
            "Epoch 164/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.3840 - accuracy: 0.8565\n",
            "Epoch 165/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.3732 - accuracy: 0.8602\n",
            "Epoch 166/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.3695 - accuracy: 0.8629\n",
            "Epoch 167/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.3659 - accuracy: 0.8634\n",
            "Epoch 168/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.3659 - accuracy: 0.8637\n",
            "Epoch 169/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.3664 - accuracy: 0.8645\n",
            "Epoch 170/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.3675 - accuracy: 0.8632\n",
            "Epoch 171/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.3686 - accuracy: 0.8643\n",
            "Epoch 172/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.3667 - accuracy: 0.8631\n",
            "Epoch 173/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.3578 - accuracy: 0.8665\n",
            "Epoch 174/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.3533 - accuracy: 0.8682\n",
            "Epoch 175/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.3605 - accuracy: 0.8664\n",
            "Epoch 176/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.3585 - accuracy: 0.8663\n",
            "Epoch 177/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.3621 - accuracy: 0.8671\n",
            "Epoch 178/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.3479 - accuracy: 0.8711\n",
            "Epoch 179/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.3524 - accuracy: 0.8677\n",
            "Epoch 180/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.3522 - accuracy: 0.8697\n",
            "Epoch 181/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.3554 - accuracy: 0.8686\n",
            "Epoch 182/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.3471 - accuracy: 0.8716\n",
            "Epoch 183/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.3563 - accuracy: 0.8694\n",
            "Epoch 184/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.3426 - accuracy: 0.8753\n",
            "Epoch 185/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.3460 - accuracy: 0.8723\n",
            "Epoch 186/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.3479 - accuracy: 0.8718\n",
            "Epoch 187/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.3451 - accuracy: 0.8739\n",
            "Epoch 188/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.3493 - accuracy: 0.8725\n",
            "Epoch 189/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.3425 - accuracy: 0.8746\n",
            "Epoch 190/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.3411 - accuracy: 0.8751\n",
            "Epoch 191/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.3388 - accuracy: 0.8767\n",
            "Epoch 192/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.3431 - accuracy: 0.8741\n",
            "Epoch 193/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.3371 - accuracy: 0.8765\n",
            "Epoch 194/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.3347 - accuracy: 0.8780\n",
            "Epoch 195/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.3300 - accuracy: 0.8787\n",
            "Epoch 196/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.3483 - accuracy: 0.8713\n",
            "Epoch 197/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.3315 - accuracy: 0.8770\n",
            "Epoch 198/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.3375 - accuracy: 0.8750\n",
            "Epoch 199/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.3340 - accuracy: 0.8767\n",
            "Epoch 200/200\n",
            "201/201 [==============================] - 2s 8ms/step - loss: 0.3417 - accuracy: 0.8753\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ff754f3e7f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HI1ZsTL7gQYE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "455911e8-9836-45a2-9943-bca6d44c822b"
      },
      "source": [
        "model.save_weights(\"/content/drive/My Drive/data_preprocessed_python/modelweight_Liking_new.h5\")\n",
        "model.save(\"/content/drive/My Drive/data_preprocessed_python/modeel_liking.h5\")\n",
        "score = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "229/229 [==============================] - 1s 2ms/step - loss: 0.4134 - accuracy: 0.8650\n",
            "Test loss: 0.4134252369403839\n",
            "Test accuracy: 0.8650273084640503\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tL8cpXrvDESE"
      },
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('/content/drive/My Drive/data_preprocessed_python/modeel_valence.h5')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vb_30mkjGH5v",
        "outputId": "b21ab963-1a7a-46d2-ef11-70a8c35a5831"
      },
      "source": [
        "score = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "229/229 [==============================] - 1s 3ms/step - loss: 0.5196 - accuracy: 0.8309\n",
            "Test loss: 0.5195998549461365\n",
            "Test accuracy: 0.8308743238449097\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gH2oBuVGDfQr"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}